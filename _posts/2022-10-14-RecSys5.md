---
layout: single
title:  "RS5 - Item2Vec and ANN"
categories : RecSys
tag : 
toc : true
use_math : true
---

# Item2Vec
ㄴ word2vec 을 추천시스템에 적용한 것

## Word2Vec

+ 임베딩 : 주어진 데이터를 낮은 차원의 vector로 표현
1) sparse representation :아이템 전체 가짓수와 차원의 수가 동일
2) dense representation :아이템 전체 가짓수보다 훨씬 작은 차원으로 표현

+ 워드 임베딩 : 단어를 벡터로 표현하는 방법

-> 단어간 의미적인 유사도 구할 수 있음

-> 임베딩 표현위한 학습 모델이 필요함(여기서는 Matrix Factorization)

+ 학습 방법
1) CBOW(Continuous Bag of words)

주변 단어로 센터의 단어 예측하는 방법

앞뒤로 몇개 쓸껀지 정해야함

 ![image-20221014132428044](/images/2022-10-14-RecSys5/image-20221014132428044.png)

학습 파라미터 $W_{V*M}$, $W'_{V*M}$

2) Skip-Gram

CBOW의 입력층과 출력층이 반대로 구성된 모델

가운데 단어로 주변 단어 이끌어 내는 것

CBOW보다 일반적으로 성능이 좋음

3) Skip-Gram with Negative Sampling (SGNS)

![image-20221014132926525](/images/2022-10-14-RecSys5/image-20221014132926525.png)

binary class classification으로 방식을 바꿈

![image-20221014143313425](/images/2022-10-14-RecSys5/image-20221014143313425.png)
1) 중심 단어를 기준으로 주변 단어들과의 내적의 sigmoid를 예측값으로 하여 0,1 분류
2) backpropagation을 통해 각 임베딩이 업데이트 되면서 모델이 수렴함
3) 최종 생성된 워드 임베딩 2개를 하나만 사용하거나 평균을 사용

## Item2Vec
(neural item embedding for collaborative filtering)

: 추천 아이템을 Word2Vec을 사용하여 임베딩
