---
layout: single
title:  "AI : Model Serving"
categories : AI_service
tag : 
toc : true
---

Serving : 모델을 앱/ 웹에 배포하는 과정, 모델 서비스화 하는 과정
Inference : 모델에 데이터가 제공되어 예측하는 경우, 사용하는 관점

+ Online Serving
+ Batch Serving
+ Edge Serving

# Online Serving
: 요청이 올 때마다 실시간으로 예측

ex) 기계 고장 예측 모델, 음식 배달 소요시간 예측

+ latency 를 최소화 하는 방법을 고려해야함 

	input 기반으로 database 데이터 추출해야하는 경우

	모델이 수행하는 연산이 무엇인지

+ 결과 값에 대한 보정 고려




## Web Server basic

client의 request -> 서버가 받음

모델 처리 -> server가 client 에게 respond

![image-20221107144207671](/images/2022-11-07-daily/image-20221107144207671.png)

## API
: Application Programming Interface

함수 잘 쓸 수 있게 정리해서 모아놓은거 (pytorch, tensorflow)

## 구현 방식

1) 직접 API 서버 개발
ex) Flask, FastAPI

2) 클라우드 서비스 활용
ex) SageMaker(AWS), Vertex AI(GCP)
단점) 다룰 줄 알아야 좋음, 비용문제

3) Serving 라이브러리 활용
ex) Tensorflow serving, torch serve, MLflow, BentoML

강의에서는 직접 개발을 해보면서 왜 이 방법을 선택하는지 알 수 있게 하는 능력을 기르겠다고 함.

# Batch Serving
: 주기적으로 학습을 하거나 예측을 하는 경우

함수 단위를 "주기적"으로 실행

ex) 
Apache Airflow


